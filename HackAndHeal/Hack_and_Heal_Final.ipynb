{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hackaton Würzburg 2025\n",
    "## Hack and Heal\n",
    "- Teammitglieder: Lukas, Rafael, Katharina \n",
    "- Technischer Mentor: Daniel\n",
    "\n",
    "### Die Challenge\n",
    "Worum gehts? \n",
    "- Livedovaskulopathie als seltene und oft fehldiagnostizierte Erkrankung \n",
    "Was sollen wir tun? \n",
    "- Training eines Algorithmus zur Unterscheidung von Livedovaskulopathie und anderen Wunden \n",
    "- Basis anhand von Fotoaufnahmen aus der Hautklinik Würzburg\n",
    "\n",
    "Langfristiges Ziel? \n",
    "- Einbinden in bestehende Wund-App der Hautklink Würzburg als Entscheidungshilfe für ÄrztInnen\n",
    "\n",
    "## Step 1.1 Daten vorverarbeiten\n",
    "Daten sichten:\n",
    "1825 Bilder, davon\n",
    "- 557 Bilder zu Livdeovaskulopathie\n",
    "- 1268 Bilder anderer Wunden\n",
    "\n",
    "Ziel: Bilder vom Trainings- und Testdaten in einheitlichem Format und Reduktion von Artefakten\n",
    "\n",
    "Vorgehen:\n",
    "- Händisches Entfernen der Daten aus dem Ordner der Livedovaskulopathie\n",
    "- Kriterien:\n",
    "  - Es muss eine Wunde vorhanden sein (Gefahr der Erkennung, dass bei Livedovaskulopathie häufig geschlossene Haut, bzw v.a. die Livedozeichnung, fotografiert wurde)\n",
    "  - Nur Bilder vom Bein (Gefahr der Erkennung, dass bei Livedovaskulopathie häufiger Wunden am Unterschenkel sind als im anderen Ordner)\n",
    "\n",
    "Einteilen des ersten Datensatzes in Trainingsdatensatz und Testdatensatz 80:20\n",
    "- Training LV (Livedovaskulopathie): 166, 1047 Nicht-LV\n",
    "- Test LV: 29, 222 Nicht-LV\n",
    "\n",
    "\n",
    "## Step 1.2 Training des Neuronalen Netzwerks: Erster Lauf\n",
    "- Verwendung des AutoML-Frameworks AucMedi, basierend auf CNN DenseNet121\n",
    "- Verwendung von Grad-CAM als ExplainableAI\n",
    "\n",
    "```bash\n",
    "aucmedi training \\\n",
    "    --path_imagedir ./data/training \\\n",
    "    --path_modeldir ./training_1/model \\\n",
    "    --epochs 500\n",
    "```\n",
    "\n",
    "Ergebnis\n",
    "- Model stoppt nach 23 Epochen da in den letzten 12 Epochen Loss function nicht besser wurde\n",
    "- Model gibt Wahrscheinlichkeiten aus\n",
    "- Decision Threshold für Klassifizuerung auf 0,5 (üblicher Wert in der Literatur) gesetzt\n",
    "- Auswertung der Qualität des Modells anhand der zurseite gelegten Test-Daten: \n",
    "\n",
    "|                      | Tatsächlich positiv | Tatsächlich negativ |\n",
    "|----------------------|---------------------|----------------------|\n",
    "| **Vorhergesagt positiv** | True Positive: 29     | False Positive: 31     |\n",
    "| **Vorhergesagt negativ** | False Negative: 0     | True Negative: 191     |\n",
    "\n",
    "  + Berechnung F1-Score: 0.65\n",
    "  + Sensitivität: 1.0\n",
    "\n",
    "## Step 1.3 Auswertung des trainierten Modells inkl. GradCam-Visualisierung\n",
    "Dann: Einfärbung der klassifizierten Bilder mit Hilfe von GradCAM (Gradient-weitghted Class Activation Mapping)\n",
    "-> Sichtbarmachung, welche Bildbrereiche das Neuronale Netz bei der Klassifikatoin besonders berücksichtigt hat (explainable AI)\n",
    "\n",
    "```bash\n",
    "aucmedi prediciton \\\n",
    "--path_modeldir ./training_1/model \\\n",
    "--path_imagedir ./data/test/LV_AND_NOT_LV \\\n",
    "--path_pred ./training_1/preds.csv \\\n",
    "--xai_method gradcam \\\n",
    "--xai_directory ./training_1/waermebilder\n",
    "```\n",
    " Ergebnis:\n",
    " - Es wurde in ein paar Fällen neben dem Hauptmerkmal (Wunde) auch Objekte wie Maßlineal oder Handschuh im Bild stark berücksichtigt, das v.a. im Ordner der nicht-LV-Wunden häufiger vorkommt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 Erneute Datenvorerarbeitung und neuer Trainingsversuch\n",
    "\n",
    "### Step 2.1. Daten per Skript bereinigen\n",
    "- automatisiertes Zuschneiden der Bilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "wound_data = pd.read_csv('/Volumes/teddoww/UlceraScan/Koordinaten_Segmentierte_Wunden.csv')\n",
    "\n",
    "clean_wound_data = wound_data.drop(columns=['id', 'image_id', 'hash', 'comment', 'updated_at'])\n",
    "clean_wound_data = clean_wound_data[clean_wound_data['label'] == 'Wound']\n",
    "clean_wound_data = clean_wound_data[clean_wound_data['source'] == 'manual']\n",
    "clean_wound_data = clean_wound_data.drop(columns=['source', 'label'])\n",
    "clean_wound_data['image_path'] = clean_wound_data['image_path'].str.split('/').str[-1]\n",
    "clean_wound_data.set_index('image_path', inplace=True)\n",
    "\n",
    "def handle_single_label(center_x: float, center_y: float, file_path: Path, save_path: Path):\n",
    "    img = Image.open(file_path)\n",
    "    width, height = img.size\n",
    "\n",
    "    save_path = save_path / file_path.parent.name / (file_path.stem + '.png')\n",
    "\n",
    "    # Convert relative center to pixel coordinates\n",
    "    cx = int(center_x * width)\n",
    "    cy = int(center_y * height)\n",
    "\n",
    "    crop_size = 1024\n",
    "    half_crop = crop_size // 2\n",
    "\n",
    "    # Initial crop box\n",
    "    left = cx - half_crop\n",
    "    upper = cy - half_crop\n",
    "    right = cx + half_crop\n",
    "    lower = cy + half_crop\n",
    "\n",
    "    # Check if crop fits inside image bounds\n",
    "    if left >= 0 and upper >= 0 and right <= width and lower <= height:\n",
    "        crop_box = (left, upper, right, lower)\n",
    "        cropped = img.crop(crop_box)\n",
    "        cropped.save(save_path)\n",
    "        return\n",
    "\n",
    "    # If none of the above fit directly, try shifting the 1024x1024 crop into bounds\n",
    "    left = max(0, min(cx - half_crop, width - crop_size))\n",
    "    upper = max(0, min(cy - half_crop, height - crop_size))\n",
    "    right = left + crop_size\n",
    "    lower = upper + crop_size\n",
    "\n",
    "    crop_box = (left, upper, right, lower)\n",
    "    cropped = img.crop(crop_box)\n",
    "    cropped.save(save_path)\n",
    "\n",
    "def handle_multi_label(df: pd.DataFrame, file_path: Path, save_path: Path):\n",
    "    xmin = df['xmin'].min()\n",
    "    xmax = df['xmax'].max()\n",
    "    ymin = df['ymin'].min()\n",
    "    ymax = df['ymax'].max()\n",
    "\n",
    "    center_x = (xmin + xmax) / 2\n",
    "    center_y = (ymin + ymax) / 2\n",
    "\n",
    "    handle_single_label(\n",
    "        center_x,\n",
    "        center_y,\n",
    "        file_path,\n",
    "        save_path\n",
    "    )\n",
    "\n",
    "def lockup_image(image_path, save_path):\n",
    "    try:\n",
    "        wound_data = clean_wound_data.loc[image_path.name]\n",
    "    except KeyError:\n",
    "        return None\n",
    "\n",
    "    if type(wound_data) is pd.DataFrame:\n",
    "        handle_multi_label(wound_data, image_path, save_path)\n",
    "    else:\n",
    "        handle_single_label(\n",
    "            (wound_data.xmax + wound_data.xmin) / 2,\n",
    "            (wound_data.ymax + wound_data.ymin) / 2,\n",
    "            image_path,\n",
    "            save_path\n",
    "        )\n",
    "\n",
    "def crop_img_data(base_path, save_path):\n",
    "    for class_name in [\"LV\", \"NOT_LV\"]:\n",
    "        class_path = base_path / class_name\n",
    "        for file_path in tqdm(class_path.iterdir(), desc=f'Cropping {class_path}'):\n",
    "            if file_path.is_file():\n",
    "                lockup_image(file_path, save_path)\n",
    "\n",
    "def init_dirs(save_path: Path):\n",
    "    save_path.mkdir(exist_ok=True)\n",
    "\n",
    "    for class_name in [\"LV\", \"NOT_LV\"]:\n",
    "        (save_path / class_name).mkdir(exist_ok=True)\n",
    "\n",
    "base_path = Path('/Volumes/teddoww/UlceraScan/')\n",
    "\n",
    "clean_test_path = base_path / 'Clean_Test_Data'\n",
    "clean_train_path = base_path / 'Clean_Training_Data'\n",
    "\n",
    "noisy_test_path = base_path / 'Test_Data'\n",
    "noisy_train_path = base_path / 'Training_Data'\n",
    "\n",
    "init_dirs(clean_test_path)\n",
    "init_dirs(clean_train_path)\n",
    "\n",
    "crop_img_data(noisy_test_path, clean_test_path)\n",
    "#crop_img_data(noisy_train_path, clean_train_path)\n",
    "\n",
    "### Disclaimer: Code from Mentor Daniel\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Modell erneut triainieren\n",
    "- Verwendung des AutoML-Frameworks AucMedi\n",
    "```bash\n",
    "aucmedi training \\\n",
    "    --path_imagedir ./data/cropped/training \\\n",
    "    --path_modeldir ./training_2/model \\\n",
    "    --epochs 500\n",
    "```\n",
    "\n",
    "Ergebnis\n",
    "- Stopp nach 42 Epochen\n",
    "- Decision Threshold auf 0,5 gesetzt\n",
    "\n",
    "### 2.3 Modell auswerten\n",
    "```bash\n",
    "aucmedi prediciton \\\n",
    "--path_modeldir ./training_2/model \\\n",
    "--path_imagedir ./data/cropped/test/LV_AND_NOT_LV \\\n",
    "--path_pred ./training_2/preds.csv \\\n",
    "--xai_method gradcam \\\n",
    "--xai_directory ./training_2/waermebilder\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.6666666666666666)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###################\n",
    "# MODELAUSWERTUNG #\n",
    "###################\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "ROOT = Path(\".\")\n",
    "results_dir = ROOT / \"training_2\"\n",
    "data_dir = ROOT / \"data\" / \"cropped\"\n",
    "\n",
    "# determine which samples should have been labeled as \"lv\" through filenames in test/LV\n",
    "actual_lv_samples = [f.stem for f in (data_dir / \"test\" / \"LV\").iterdir()]\n",
    "\n",
    "df = pd.read_csv(results_dir / \"preds.csv\")\n",
    "df[\"pred_lv\"] = df[\"LV\"] > 0.5\n",
    "df[\"actual_lv\"] = df[\"SAMPLE\"].map(\n",
    "    lambda x: x in actual_lv_samples\n",
    ")\n",
    "df = df.drop(columns=[\"LV\", \"NOT_LV\"])\n",
    "#\n",
    "TP = ( df.pred_lv &  df.actual_lv).sum()\n",
    "TN = (~df.pred_lv & ~df.actual_lv).sum()\n",
    "FP = ( df.pred_lv & ~df.actual_lv).sum()\n",
    "FN = (~df.pred_lv &  df.actual_lv).sum()\n",
    "\n",
    "#calculate F1 score\n",
    "f1_score = 2*TP / (2*TP + FP + FN) # 0.67\n",
    "f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1 score beträgt 0,67."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual_true</th>\n",
       "      <th>actual_false</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pred_true</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pred_false</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            actual_true  actual_false\n",
       "pred_true             4             2\n",
       "pred_false            2            20"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Erstellung der Konfusionsmatrix\n",
    "confusion_matrix = pd.DataFrame(\n",
    "    {\n",
    "        \"actual_true\": [TP, FN],\n",
    "        \"actual_false\": [FP, TN],\n",
    "    },\n",
    "    index = [\"pred_true\", \"pred_false\"]\n",
    ")\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SAMPLE</th>\n",
       "      <th>pred_lv</th>\n",
       "      <th>actual_lv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3716_17_ulc025_14112017_2</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3909_15_ulc033_21122015_3</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4346_20_43_05102020_4</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4455_18_37_11102018_5</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4545_22_ulc002_14112022_4</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      SAMPLE  pred_lv  actual_lv\n",
       "0  3716_17_ulc025_14112017_2    False      False\n",
       "1  3909_15_ulc033_21122015_3    False      False\n",
       "2      4346_20_43_05102020_4    False      False\n",
       "3      4455_18_37_11102018_5    False      False\n",
       "4  4545_22_ulc002_14112022_4    False      False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Gegenüberstellung Vorhersagen vs tatsächliches Label (Ausschnitt)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Fazit\n",
    "- Wir haben ein Modell trainiert, welches zur automatisierten Erkennung von Livedovaskulopathie genutzt werden kann.\n",
    "\n",
    "#### Problem der Grunddaten\n",
    "- **Bildartefakte** wie Hintergrundobjekte, Lineale oder Handschuhe traten im ersten Datensatz häufiger bei einer bestimmten Erkrankungsart auf.  \n",
    "  → Gefahr: Das Modell könnte diese Artefakte als krankheitsspezifische Merkmale fehlinterpretieren.  \n",
    "  → Im zweiten Training wurden diese Artefakte weitgehend entfernt, was zu realistischeren Lernergebnissen führt.\n",
    "\n",
    "- **Wundgröße** zeigt eine Korrelation mit der Diagnose \"LV vs. Nicht-LV\"\n",
    "  → Möglicher Bias, wenn das Modell Größe als Hauptunterscheidungsmerkmal nutzt.\n",
    "\n",
    "#### Vergleich der beiden Trainings\n",
    "| **Training**        | **Sensitivität** | **Spezifität** | **F1-Score** |\n",
    "|---------------------|------------------|----------------|--------------|\n",
    "| Erstes Training     | 100 %            | 89 %           | 0,65         |\n",
    "| Zweites Training    | 67 %             | 91 %           | 0,67         |\n",
    "\n",
    "\n",
    "\n",
    "Interpretation der Ergebnisse\n",
    "- Der Ergebnis-Score hat sich im zweiten Training theoretisch verschlechtert, insbesondere die Sensitivität sank von 100 % auf 67 %.  \n",
    "  → Auf den ersten Blick könnte man vermuten, dass der ursprüngliche Datensatz besser geeignet gewesen wäre.  \n",
    "  → **Diese Schlussfolgerung wäre jedoch irreführend**, da:\n",
    "  - Overfitting auf den Trainingsdatensatz \n",
    "  - Das Modell im ersten Training möglicherweise noch deutlicher Artefakte im Bild (z. B. Handschuhe, Lineale, Hintergrund) als relevante Merkmale gelernt hat – ein klassischer Fall von Overfitting auf irrelevante Bildinformationen.\n",
    "\n",
    "- Zudem: unklar, wie das Modell mit Grenzfällen umgeht, da der Datensatz hauptsächlich klar diagnostizierte Fälle enthielt.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
