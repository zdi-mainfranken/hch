{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_init_from_model: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Bern ist die Hauptstadt der Schweiz.\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "#!pip install -q torch transformers huggingface_hub accelerate llama-cpp-python[cuda]\n",
    "\n",
    "# Download Mistral 7B (4-bit quantized GGUF)\n",
    "#!wget -q https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf -O mistral-7b.Q4_K_M.gguf\n",
    "\n",
    "# Load model and run inference\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# Initialize the model\n",
    "llm = Llama(model_path=\"mistral-7b-instruct-v0.1.Q4_K_M.gguf\", n_gpu_layers=35, n_ctx=4096, verbose=False)\n",
    "\n",
    "# Run a simple German QA example\n",
    "question = \"Was ist die Hauptstadt der Schweiz?\"\n",
    "response = llm(f\"Antworte auf Deutsch: {question}\")\n",
    "\n",
    "# Print response\n",
    "print(response[\"choices\"][0][\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Bern ist die Hauptstadt der Schweiz.\n",
      "{'id': 'cmpl-3c27f45c-4fcf-48a4-a88a-d6bb4d41afa6', 'object': 'text_completion', 'created': 1743093632, 'model': 'mistral-7b-instruct-v0.1.Q4_K_M.gguf', 'choices': [{'text': \"Great! Let's move on to step 2. Please position and connect the CARDIOHELP-i to the holder and any necessary accessories.\", 'index': 0, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 205, 'completion_tokens': 32, 'total_tokens': 237}}\n"
     ]
    }
   ],
   "source": [
    "# Load model and run inference\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# Initialize the model\n",
    "llm = Llama(model_path=\"mistral-7b-instruct-v0.1.Q4_K_M.gguf\", n_gpu_layers=35, n_ctx=32768, verbose=False)\n",
    "\n",
    "# Run a simple German QA example\n",
    "question = \"Was ist die Hauptstadt der Schweiz?\"\n",
    "response = llm(f\"Antworte auf Deutsch: {question}\")\n",
    "\n",
    "# Print response\n",
    "print(response[\"choices\"][0][\"text\"])\n",
    "\n",
    "# System prompt with instructions for the workflow assistant\n",
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant for setting up the CARDIOHELP-i medical device. \n",
    "Follow these rules:\n",
    "1. Detect the user's language and respond in the same language\n",
    "2. Guide the user through the setup process step by step\n",
    "3. Only provide the next step after the user confirms with \"ok\" or equivalent in their language\n",
    "4. Be concise and clear with instructions\n",
    "5. Maintain a professional, supportive tone\n",
    "\n",
    "The setup workflow has the following steps:\n",
    "1. Mount the holder and any necessary accessories\n",
    "2. Position and connect the CARDIOHELP-i\n",
    "3. Connect the sensors\n",
    "4. Turn on the CARDIOHELP-i and allow it to run a self-test\n",
    "\n",
    "Wait for user confirmation after each step before proceeding to the next one.\n",
    "\"\"\"\n",
    "\n",
    "current_step = 0\n",
    "user_input   = \"ok\"\n",
    "\n",
    "\n",
    "\n",
    "prompt = f\"\"\"[INST] {system_prompt}\n",
    "\n",
    "Current status: Step {current_step+1} of 5\n",
    "User input: {user_input} [/INST]\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Create the prompt for the model\n",
    "#prompt = f\"\"\"<s>[INST] {system_prompt}\n",
    "\n",
    "#Current status: Step {current_step+1} of 5\n",
    "#User input: {user_input} [/INST]\n",
    "#\"\"\"\n",
    "#Get model response\n",
    "\n",
    "response = llm(prompt, max_tokens=512, temperature=0.1)\n",
    "model_response = response[\"choices\"][0][\"text\"].strip()\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Hello! I'm here to assist you with setting up the CARDIOHELP medical device. Let's start with step 1. Please mount the holder and any necessary accessories. Once you've completed this step, please let me know and we can move on to the next step.\n",
      "-- user: ok\n",
      "Step 2: Hello! I'm here to help you set up the CARDIOHELP medical device. Let's start with step 2. Please position the device on a flat surface and connect it to the power source. Make sure the device is securely plugged in and that the power cord is not damaged. Once you have connected the device, please confirm with \"ok\" or equivalent in your language.\n",
      "-- user: ok\n",
      "Step 3: Hello! I'm here to assist you with setting up the CARDIOHELP medical device. We're currently on step 3 of 5. Let's move on to connecting the sensors.\n",
      "\n",
      "To connect the sensors, please follow these steps:\n",
      "\n",
      "1. Locate the sensor pads on the device. They should be labeled \"front\" and \"back.\"\n",
      "2. Place the sensor pads on the appropriate locations on your body. The front pad should be placed on your chest, just below the collarbone, and the back pad should be placed on your lower back.\n",
      "3. Press the sensor pads firmly onto your skin to ensure a good connection.\n",
      "4. Turn on the device and wait for the sensors to calibrate. This may take a few minutes.\n",
      "5. Once the sensors have calibrated, you should see a green light on the device indicating that it is ready to use.\n",
      "\n",
      "Ok, are you ready to move on to step 4?\n",
      "-- user: ok\n",
      "Step 4: Hello! I'm here to help you set up the CARDIOHELP medical device. Let's get started!\n",
      "\n",
      "Step 1: Detect the user's language and respond in the same language\n",
      "\n",
      "Step 2: Guide the user through the setup process step by step\n",
      "\n",
      "Step 3: Only provide the next step after the user confirms with \"ok\" or equivalent in their language\n",
      "\n",
      "Step 4: Turn on the CARDIOHELP and allow it to run a self-test\n",
      "\n",
      "Please turn on the CARDIOHELP and allow it to run a self-test. Once the self-test is complete, please confirm with \"ok\" or equivalent in your language.\n",
      "-- user: ok\n"
     ]
    }
   ],
   "source": [
    "# Load model and run inference\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# Initialize the model\n",
    "llm = Llama(model_path=\"mistral-7b-instruct-v0.1.Q4_K_M.gguf\", n_gpu_layers=35, n_ctx=32768, verbose=False)\n",
    "#llm = Llama(model_path=\"gemma-2-2b-it-Q4_K_M.gguf\", verbose=False)\n",
    "\n",
    "# Define the workflow with dynamic step-by-step guidance\n",
    "workflow_steps = [\n",
    "    \n",
    "    \"Mount the holder and any necessary accessories\",\n",
    "    \"Position and connect the CARDIOHELP\",\n",
    "    \"Connect the sensors\",\n",
    "    \"Turn on the CARDIOHELP and allow it to run a self-test\"\n",
    "]\n",
    "\n",
    "current_step = 0\n",
    "user_language = \"english\"  # You might want to update this dynamically with user input\n",
    "\n",
    "while current_step < len(workflow_steps):\n",
    "\n",
    "    step_instruction = workflow_steps[current_step]\n",
    "    \n",
    "    # System prompt with instructions for the workflow assistant\n",
    "    system_prompt = f\"\"\"\n",
    "    You are a helpful assistant for setting up the CARDIOHELP medical device.\n",
    "    Follow these rules:\n",
    "    1. Detect the user's language and respond in the same language.\n",
    "    2. Guide the user through the setup process step by step\n",
    "    3. Only provide the next step after the user confirms with \"ok\".\n",
    "    4. Be concise and clear with instructions\n",
    "    5. Maintain a professional, supportive tone and friendly.\n",
    "\n",
    "    Current status: Step {current_step + 1} of 5\n",
    "    User language: {user_language}\n",
    "    only introduce yourself if you are in currect status 1\n",
    "    \"\"\"\n",
    "\n",
    "    # Prompt to provide the current step's instruction\n",
    "    prompt = f\"\"\"[INST] {system_prompt} Instruction for step {current_step + 1}: {step_instruction} [/INST]\"\"\"\n",
    "\n",
    "    # Get model response\n",
    "    response = llm(prompt, max_tokens=512, temperature=0.1)\n",
    "    model_response = response[\"choices\"][0][\"text\"].strip()\n",
    "\n",
    "    # Print the response for the current step\n",
    "    print(f\"Step {current_step + 1}: {model_response}\")\n",
    "\n",
    "    # Ask for user confirmation before proceeding\n",
    "\n",
    "    # call to funtion to read users voice\n",
    "    #user_input = read_users_voice()    # this funtion delivers a charachterschain\n",
    "    # user_input = input(\"Please confirm (type 'ok' to continue): \").strip()\n",
    "\n",
    "    user_input = 'ok'\n",
    "    \n",
    "    print('-- user: ok')\n",
    "\n",
    "    if user_input.lower() in [\"ok\", \"yes\", \"ja\", \"oui\"]:  # Adjust for multiple languages\n",
    "        current_step += 1\n",
    "    else:   \n",
    "        print(\"Please confirm to proceed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Hello! I'm here to help you set up your CARDIOHELP medical device. Let's start with step 1. Please mount the holder and any necessary accessories. Make sure to follow the manufacturer's instructions carefully. Once you've completed this step, please let me know and we can move on to the next step.\n",
      "-- user: finish\n",
      "DEBUG: Model interpreted user response as -> 'no.'\n",
      "Please complete this step before continuing.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 60\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;66;03m# Generate model response for the step instruction\u001b[39;00m\n\u001b[32m     59\u001b[39m prompt = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\u001b[33m[INST] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msystem_prompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Instruction for step \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_step\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep_instruction\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m [/INST]\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m response = \u001b[43mllm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     61\u001b[39m model_response = response[\u001b[33m\"\u001b[39m\u001b[33mchoices\u001b[39m\u001b[33m\"\u001b[39m][\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m].strip()\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# Print the response for the current step\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\NIKO9\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\llama_cpp\\llama.py:1902\u001b[39m, in \u001b[36mLlama.__call__\u001b[39m\u001b[34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[39m\n\u001b[32m   1838\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\n\u001b[32m   1839\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1840\u001b[39m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1864\u001b[39m     logit_bias: Optional[Dict[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1865\u001b[39m ) -> Union[CreateCompletionResponse, Iterator[CreateCompletionStreamResponse]]:\n\u001b[32m   1866\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Generate text from a prompt.\u001b[39;00m\n\u001b[32m   1867\u001b[39m \n\u001b[32m   1868\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1900\u001b[39m \u001b[33;03m        Response object containing the generated text.\u001b[39;00m\n\u001b[32m   1901\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1902\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcreate_completion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1903\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1904\u001b[39m \u001b[43m        \u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m=\u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1905\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1906\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1907\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1908\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmin_p\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmin_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1909\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtypical_p\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtypical_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1910\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1911\u001b[39m \u001b[43m        \u001b[49m\u001b[43mecho\u001b[49m\u001b[43m=\u001b[49m\u001b[43mecho\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1912\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1913\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1914\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1915\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepeat_penalty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepeat_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1916\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1917\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1918\u001b[39m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1919\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtfs_z\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtfs_z\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1920\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmirostat_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmirostat_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1921\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmirostat_tau\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmirostat_tau\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1922\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmirostat_eta\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmirostat_eta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1923\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1924\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1925\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1926\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrammar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrammar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1927\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1928\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\NIKO9\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\llama_cpp\\llama.py:1835\u001b[39m, in \u001b[36mLlama.create_completion\u001b[39m\u001b[34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[39m\n\u001b[32m   1833\u001b[39m     chunks: Iterator[CreateCompletionStreamResponse] = completion_or_chunks\n\u001b[32m   1834\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m chunks\n\u001b[32m-> \u001b[39m\u001b[32m1835\u001b[39m completion: Completion = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcompletion_or_chunks\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m   1836\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m completion\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\NIKO9\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\llama_cpp\\llama.py:1320\u001b[39m, in \u001b[36mLlama._create_completion\u001b[39m\u001b[34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[39m\n\u001b[32m   1318\u001b[39m finish_reason = \u001b[33m\"\u001b[39m\u001b[33mlength\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1319\u001b[39m multibyte_fix = \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1320\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1321\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1322\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1324\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmin_p\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmin_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtypical_p\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtypical_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1326\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1327\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtfs_z\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtfs_z\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1328\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmirostat_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmirostat_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1329\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmirostat_tau\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmirostat_tau\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1330\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmirostat_eta\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmirostat_eta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1331\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1332\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1333\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepeat_penalty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepeat_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1334\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1335\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1336\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrammar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrammar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1337\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1338\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mllama_cpp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mllama_token_is_eog\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1339\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdetokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompletion_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt_tokens\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\NIKO9\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\llama_cpp\\llama.py:912\u001b[39m, in \u001b[36mLlama.generate\u001b[39m\u001b[34m(self, tokens, top_k, top_p, min_p, typical_p, temp, repeat_penalty, reset, frequency_penalty, presence_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, penalize_nl, logits_processor, stopping_criteria, grammar)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;66;03m# Eval and sample\u001b[39;00m\n\u001b[32m    911\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m912\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    913\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m sample_idx < \u001b[38;5;28mself\u001b[39m.n_tokens:\n\u001b[32m    914\u001b[39m         token = \u001b[38;5;28mself\u001b[39m.sample(\n\u001b[32m    915\u001b[39m             top_k=top_k,\n\u001b[32m    916\u001b[39m             top_p=top_p,\n\u001b[32m   (...)\u001b[39m\u001b[32m    930\u001b[39m             idx=sample_idx,\n\u001b[32m    931\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\NIKO9\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\llama_cpp\\llama.py:646\u001b[39m, in \u001b[36mLlama.eval\u001b[39m\u001b[34m(self, tokens)\u001b[39m\n\u001b[32m    642\u001b[39m n_tokens = \u001b[38;5;28mlen\u001b[39m(batch)\n\u001b[32m    643\u001b[39m \u001b[38;5;28mself\u001b[39m._batch.set_batch(\n\u001b[32m    644\u001b[39m     batch=batch, n_past=n_past, logits_all=\u001b[38;5;28mself\u001b[39m.context_params.logits_all\n\u001b[32m    645\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m646\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[38;5;66;03m# Save tokens\u001b[39;00m\n\u001b[32m    648\u001b[39m \u001b[38;5;28mself\u001b[39m.input_ids[n_past : n_past + n_tokens] = batch\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\NIKO9\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\llama_cpp\\_internals.py:306\u001b[39m, in \u001b[36mLlamaContext.decode\u001b[39m\u001b[34m(self, batch)\u001b[39m\n\u001b[32m    305\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch: LlamaBatch):\n\u001b[32m--> \u001b[39m\u001b[32m306\u001b[39m     return_code = \u001b[43mllama_cpp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mllama_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    307\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    308\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    309\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    310\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m return_code != \u001b[32m0\u001b[39m:\n\u001b[32m    311\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mllama_decode returned \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreturn_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Load model and run inference\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# Initialize the model\n",
    "llm = Llama(model_path=\"mistral-7b-instruct-v0.1.Q4_K_M.gguf\", n_gpu_layers=35, n_ctx=32768, verbose=False)\n",
    "\n",
    "# Define the workflow steps\n",
    "workflow_steps = [\n",
    "    \"Mount the holder and any necessary accessories\",\n",
    "    \"Position and connect the CARDIOHELP\",\n",
    "    \"Connect the sensors\",\n",
    "    \"Turn on the CARDIOHELP and allow it to run a self-test\"\n",
    "]\n",
    "\n",
    "current_step = 0\n",
    "user_language = \"English\"  # This could be dynamically detected\n",
    "\n",
    "def interpret_user_response(user_input):\n",
    "    \"\"\"Ask the model if the user confirmed the step and enforce a strict 'yes' or 'no' answer.\"\"\"\n",
    "    confirmation_prompt = f\"\"\"\n",
    "    You are an intelligent assistant guiding a user through setting up the CARDIOHELP medical device.\n",
    "    The user just responded: \"{user_input}\"\n",
    "    \n",
    "    Does this mean they have completed the step?\n",
    "    Answer ONLY \"yes\" or \"no\". Do NOT include any other text.\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm(f\"[INST] {confirmation_prompt} [/INST]\", max_tokens=2, temperature=0.1)\n",
    "    model_reply = response[\"choices\"][0][\"text\"].strip().lower()\n",
    "\n",
    "    print(f\"DEBUG: Model interpreted user response as -> '{model_reply}'\")  # Debugging\n",
    "\n",
    "    if model_reply.startswith(\"yes\"):\n",
    "        return \"yes\"\n",
    "    elif model_reply.startswith(\"no\"):\n",
    "        return \"no\"\n",
    "    else:\n",
    "        return \"unclear\"  # Handle unexpected responses\n",
    "\n",
    "while current_step < len(workflow_steps):\n",
    "\n",
    "    step_instruction = workflow_steps[current_step]\n",
    "    \n",
    "    # System prompt with instructions\n",
    "    system_prompt = f\"\"\"\n",
    "    You are a helpful assistant for setting up the CARDIOHELP medical device.\n",
    "    Follow these rules:\n",
    "    1. Detect the user's language and respond in the same language\n",
    "    2. Guide the user through the setup process step by step\n",
    "    3. Only provide the next step after the user confirms\n",
    "    4. Be concise and clear with instructions\n",
    "    5. Maintain a professional, supportive, and friendly tone\n",
    "\n",
    "    Current status: Step {current_step + 1} of {len(workflow_steps)}\n",
    "    User language: {user_language}\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate model response for the step instruction\n",
    "    prompt = f\"\"\"[INST] {system_prompt} Instruction for step {current_step + 1}: {step_instruction} [/INST]\"\"\"\n",
    "    response = llm(prompt, max_tokens=512, temperature=0.1)\n",
    "    model_response = response[\"choices\"][0][\"text\"].strip()\n",
    "\n",
    "    # Print the response for the current step\n",
    "    print(f\"Step {current_step + 1}: {model_response}\")\n",
    "\n",
    "    # Simulated voice input function\n",
    "    # user_input = read_users_voice()\n",
    "    #user_input = input(\"Your response: \").strip()  # Replace with voice processing in the future\n",
    "    user_input = \"finished\"\n",
    "\n",
    "    print(f\"-- user: {user_input}\")\n",
    "\n",
    "    # Let the model decide if we move to the next step\n",
    "    model_decision = interpret_user_response(user_input)\n",
    "    \n",
    "    if model_decision == \"yes\":\n",
    "        current_step += 1\n",
    "    elif model_decision == \"no\":\n",
    "        print(\"Please complete this step before continuing.\")\n",
    "    else:\n",
    "        print(\"I'm not sure if you confirmed. Please answer clearly.\")\n",
    "\n",
    "print(\"Setup complete! âœ…\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from em_german_7b_v01.Q8_0.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 7\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q8_0:  226 tensors\n",
      "print_info: file format = GGUF V2\n",
      "print_info: file type   = Q8_0\n",
      "print_info: file size   = 6.67 GiB (8.50 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control token:      2 '</s>' is not marked as EOG\n",
      "load: control token:      1 '<s>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: special tokens cache size = 3\n",
      "load: token to piece cache size = 0.1684 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 4096\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 32\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 1\n",
      "print_info: n_embd_k_gqa     = 4096\n",
      "print_info: n_embd_v_gqa     = 4096\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 11008\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 10000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 4096\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 7B\n",
      "print_info: model params     = 6.74 B\n",
      "print_info: general.name     = LLaMA v2\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 32000\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 1 '<s>'\n",
      "print_info: EOS token        = 2 '</s>'\n",
      "print_info: UNK token        = 0 '<unk>'\n",
      "print_info: LF token         = 13 '<0x0A>'\n",
      "print_info: EOG token        = 2 '</s>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU\n",
      "load_tensors: layer   1 assigned to device CPU\n",
      "load_tensors: layer   2 assigned to device CPU\n",
      "load_tensors: layer   3 assigned to device CPU\n",
      "load_tensors: layer   4 assigned to device CPU\n",
      "load_tensors: layer   5 assigned to device CPU\n",
      "load_tensors: layer   6 assigned to device CPU\n",
      "load_tensors: layer   7 assigned to device CPU\n",
      "load_tensors: layer   8 assigned to device CPU\n",
      "load_tensors: layer   9 assigned to device CPU\n",
      "load_tensors: layer  10 assigned to device CPU\n",
      "load_tensors: layer  11 assigned to device CPU\n",
      "load_tensors: layer  12 assigned to device CPU\n",
      "load_tensors: layer  13 assigned to device CPU\n",
      "load_tensors: layer  14 assigned to device CPU\n",
      "load_tensors: layer  15 assigned to device CPU\n",
      "load_tensors: layer  16 assigned to device CPU\n",
      "load_tensors: layer  17 assigned to device CPU\n",
      "load_tensors: layer  18 assigned to device CPU\n",
      "load_tensors: layer  19 assigned to device CPU\n",
      "load_tensors: layer  20 assigned to device CPU\n",
      "load_tensors: layer  21 assigned to device CPU\n",
      "load_tensors: layer  22 assigned to device CPU\n",
      "load_tensors: layer  23 assigned to device CPU\n",
      "load_tensors: layer  24 assigned to device CPU\n",
      "load_tensors: layer  25 assigned to device CPU\n",
      "load_tensors: layer  26 assigned to device CPU\n",
      "load_tensors: layer  27 assigned to device CPU\n",
      "load_tensors: layer  28 assigned to device CPU\n",
      "load_tensors: layer  29 assigned to device CPU\n",
      "load_tensors: layer  30 assigned to device CPU\n",
      "load_tensors: layer  31 assigned to device CPU\n",
      "load_tensors: layer  32 assigned to device CPU\n",
      "load_tensors: tensor 'token_embd.weight' (q8_0) (and 290 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors:   CPU_Mapped model buffer size =  6828.64 MiB\n",
      "...................................................................................................\n",
      "llama_init_from_model: n_seq_max     = 1\n",
      "llama_init_from_model: n_ctx         = 4096\n",
      "llama_init_from_model: n_ctx_per_seq = 4096\n",
      "llama_init_from_model: n_batch       = 512\n",
      "llama_init_from_model: n_ubatch      = 512\n",
      "llama_init_from_model: flash_attn    = 0\n",
      "llama_init_from_model: freq_base     = 10000.0\n",
      "llama_init_from_model: freq_scale    = 1\n",
      "llama_kv_cache_init: kv_size = 4096, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 16: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 17: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 18: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 19: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 20: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 21: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 22: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 23: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 24: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 25: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 26: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 27: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 28: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 29: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 30: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 31: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init:        CPU KV buffer size =  2048.00 MiB\n",
      "llama_init_from_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\n",
      "llama_init_from_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_init_from_model:        CPU compute buffer size =   296.01 MiB\n",
      "llama_init_from_model: graph nodes  = 1030\n",
      "llama_init_from_model: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'general.name': 'LLaMA v2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '11008', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '7', 'llama.attention.head_count_kv': '32', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0'}\n",
      "Using fallback chat format: llama-2\n",
      "llama_perf_context_print:        load time =   10858.49 ms\n",
      "llama_perf_context_print: prompt eval time =   10857.47 ms /   192 tokens (   56.55 ms per token,    17.68 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   10860.67 ms /   193 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 108 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1: \n",
      "-- user: abgeschlossen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   10858.49 ms\n",
      "llama_perf_context_print: prompt eval time =    5641.00 ms /   108 tokens (   52.23 ms per token,    19.15 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5643.21 ms /   109 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 186 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   10858.49 ms\n",
      "llama_perf_context_print: prompt eval time =    9673.03 ms /   186 tokens (   52.01 ms per token,    19.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    9674.15 ms /   187 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 107 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1: \n",
      "-- user: fertig\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   10858.49 ms\n",
      "llama_perf_context_print: prompt eval time =    6414.49 ms /   107 tokens (   59.95 ms per token,    16.68 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6415.31 ms /   108 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 186 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   10858.49 ms\n",
      "llama_perf_context_print: prompt eval time =   12647.81 ms /   186 tokens (   68.00 ms per token,    14.71 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   12648.90 ms /   187 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 107 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1: \n",
      "-- user: fertig\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   10858.49 ms\n",
      "llama_perf_context_print: prompt eval time =   29646.00 ms /   107 tokens (  277.07 ms per token,     3.61 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   29647.10 ms /   108 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 186 prompt tokens to eval\n"
     ]
    }
   ],
   "source": [
    "# Load model and run inference\n",
    "import random\n",
    "from llama_cpp import Llama\n",
    "import re\n",
    "\n",
    "# Initialize the model\n",
    "llm = Llama(model_path=\"mistral-7b-instruct-v0.1.Q4_K_M.gguf\", n_gpu_layers=35, n_ctx=32768, verbose=False)\n",
    "#llm = Llama(model_path=\"em_german_7b_v01.Q8_0.gguf\", n_ctx= 4096,verbose=False )\n",
    "#llm = Llama(model_path=\"em_german_7b_v01.Q8_0.gguf\", n_ctx=4096, verbose=True)\n",
    "\n",
    "# Define the workflow steps\n",
    "workflow_steps = [\n",
    "    \"Mount the holder and any necessary accessories.\",\n",
    "    \"Position and connect the CARDIOHELP.\",\n",
    "    \"Connect the sensors.\",\n",
    "    \"Turn on the CARDIOHELP and allow it to run a self-test.\"\n",
    "]\n",
    "\n",
    "current_step = 0\n",
    "user_language = \"English\"  # This could be dynamically detected\n",
    "\n",
    "def interpret_user_response(user_input):\n",
    "    \"\"\"Ask the model if the user confirmed the step and enforce a strict 'yes' or 'no' answer.\"\"\"\n",
    "    confirmation_prompt = f\"\"\"\n",
    "    You are an intelligent assistant helping the user set up the CARDIOHELP medical device.\n",
    "    \n",
    "    The user responded: \"{user_input}\"\n",
    "    \n",
    "    Decide whether this means they have finished the step. Examples of confirmations:\n",
    "    - \"Done!\"\n",
    "    - \"Yes, I finished.\"\n",
    "    - \"or anything else that means finished\"\n",
    "    \n",
    "    Answer ONLY with \"yes\" or \"no\". Do NOT include any other text.\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm(f\"[INST] {confirmation_prompt} [/INST]\", max_tokens=2, temperature=0.1)\n",
    "    model_reply = response[\"choices\"][0][\"text\"].strip().lower()\n",
    "\n",
    "    # Debugging output\n",
    "    #print(f\"DEBUG: Raw model response -> '{model_reply}'\")  \n",
    "\n",
    "    # Regex pattern to check if the response starts with 'yes' or 'no'\n",
    "    if re.match(r\"^\\s*yes\\b\", model_reply):\n",
    "        return \"yes\"\n",
    "    elif re.match(r\"^\\s*no\\b\", model_reply):\n",
    "        return \"no\"\n",
    "    else:\n",
    "        return \"unclear\"  # Handle unexpected responses\n",
    "\n",
    "while current_step < len(workflow_steps):\n",
    "\n",
    "    step_instruction = workflow_steps[current_step]\n",
    "\n",
    "    #if current_step == 0:\n",
    "    \n",
    "        # System prompt with instructions\n",
    "    system_prompt = f\"\"\"\n",
    "        You are a helpful assistant for setting up the CARDIOHELP medical device.\n",
    "        just say hello if you are in the current step number 1.\n",
    "        for the rest just follow the workflow\n",
    "        Follow these rules:\n",
    "        1. Detect the user's language and respond in the same language.\n",
    "        2. Guide the user through the setup process step by step.\n",
    "        3. Only provide the next step after the user confirms.\n",
    "        4. Be concise and clear with instructions.\n",
    "        5. Maintain a professional, supportive, and friendly tone.\n",
    "\n",
    "        Current status: Step {current_step + 1} of {len(workflow_steps)}\n",
    "        User language: {user_language}\n",
    "        lastly: Detect the user's language and respond in the same language.\n",
    "        \"\"\"\n",
    "    #else:\n",
    "        \n",
    "    #system_prompt = f\"\"\"Current status: Step {current_step + 1} of {len(workflow_steps)}\n",
    "    #User language: {user_language}\n",
    "    #lastly: Detect the user's language and respond in the same language.\n",
    "    #\"\"\"\n",
    "\n",
    "    # Generate model response for the step instruction\n",
    "    prompt = f\"\"\"[INST] {system_prompt} Instruction for step {current_step + 1}: {step_instruction} [/INST]\"\"\"\n",
    "    response = llm(prompt, max_tokens=512, temperature=0.1)\n",
    "    model_response = response[\"choices\"][0][\"text\"].strip()\n",
    "\n",
    "    # Print the response for the current step\n",
    "    print(f\"\\nStep {current_step + 1}: {model_response}\")\n",
    "\n",
    "    # Simulated voice input function\n",
    "    #user_input = input(\"Your response: \").strip()  # Replace with voice processing in the future\n",
    "\n",
    "    #user_input = 'finished'\n",
    "    #user_input = random.choice([\"finished\",\"done\",\"completed\", \"ok\"])\n",
    "    user_input = random.choice([\"fertig\", \"erledigt\", \"abgeschlossen\", \"ok\"])\n",
    "\n",
    "    print(f\"-- user: {user_input}\")\n",
    "\n",
    "    # Let the model decide if we move to the next step\n",
    "    model_decision = interpret_user_response(user_input)\n",
    "    \n",
    "    if model_decision == \"yes\":\n",
    "        current_step += 1\n",
    "    elif model_decision == \"no\":\n",
    "        print(\"Please complete this step before continuing.\")\n",
    "    else:\n",
    "        print(\"I'm not sure if you confirmed. Please answer clearly (e.g., 'Done', 'Finished', 'Weiter').\")\n",
    "\n",
    "print(\"Setup complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# german (em_mistral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bitte gib Anweisungen fÃ¼r Schritt 2: Montiere den Motor und die Steuerung.\n",
      "-- Benutzer: ok\n",
      "Schritt 2: Positionieren und schlieÃŸen Sie das CARDIOHELP-GerÃ¤t an\n",
      "\n",
      "Um das CARDIOHELP-GerÃ¤t an Ihrem GerÃ¤t zu positionieren und zu schlieÃŸen, befolge diese Anweisungen:\n",
      "\n",
      "1. Stelle sicher, dass das CARDIOHELP-GerÃ¤t aus dem AufbewahrungsbehÃ¤lter entnommen und auf Ihrem GerÃ¤t liegt.\n",
      "2. Das CARDIOHELP-GerÃ¤t sollte in der horizontalen Position liegen.\n",
      "3. Die Klemmen zur Verbindung des GerÃ¤ts zum GerÃ¤t Ihres Patienten sind oben und unten sichtbar.\n",
      "4. SchlieÃŸen Sie das CARDIOHELP-GerÃ¤t an die Klemmen am GerÃ¤t Ihres Patienten.\n",
      "5. Das CARDIOHELP-GerÃ¤t sollte sicher an Ihrem GerÃ¤t angekoppelt sein.\n",
      "\n",
      "Nun ist das CARDIOHELP-GerÃ¤t an Ihrem GerÃ¤t angekoppelt und bereit, die erforder\n",
      "-- Benutzer: erledigt\n",
      "Wenn du den Anweisungen in der vorherigen ErklÃ¤rung folgst, solltest du die Sensoren wie in den Bildern unten gezeigt verbunden haben.\n",
      "-- Benutzer: ok\n",
      "Wenn Sie das CARDIOHELP-GerÃ¤t erhalten haben, folgen Sie bitte diesen Anweisungen, um es zu installieren und zu testen.\n",
      "\n",
      "Schritt 4: Schalten Sie das CARDIOHELP-GerÃ¤t ein und lassen Sie es einen Selbsttest durchfÃ¼hren\n",
      "\n",
      "1. ÃœberprÃ¼fen Sie zunÃ¤chst, ob das GerÃ¤t bereits in den angegebenen Zustand eingebaut ist.\n",
      "2. Schalten Sie das GerÃ¤t ein und verbinden Sie es mit dem USB-Port Ihres Computers.\n",
      "3. Nachdem das GerÃ¤t eingebaut ist, sollte der Self-Test automatisch gestartet werden.\n",
      "4. Der Self-Test zeigt Ihnen, ob das GerÃ¤t ordnungsgemÃ¤ÃŸ funktioniert.\n",
      "5. Sobald der Self-Test abgeschlossen ist, werden Sie eine Nachricht erhalten, die Ihnen mitteilen sollte, ob das GerÃ¤t ordnungsgemÃ¤ÃŸ funktioniert.\n",
      "\n",
      "Wenn Sie wÃ¤hrend des Self-Tests einen Fehler beobachten, sollten Sie das GerÃ¤t vorsichtig ausschalten und sicherstellen, dass\n",
      "-- Benutzer: fertig\n",
      "Einrichtung abgeschlossen!\n"
     ]
    }
   ],
   "source": [
    "# Load model and run inference\n",
    "import random\n",
    "from llama_cpp import Llama\n",
    "import re\n",
    "\n",
    "# Initialize the model with more appropriate settings\n",
    "llm = Llama(\n",
    "    model_path=\"em_german_7b_v01.Q8_0.gguf\", \n",
    "    n_ctx=4096, \n",
    "    verbose=False,\n",
    "    n_batch=512,  # Increase batch size for better performance\n",
    "    echo=False,   # Don't include the prompt in the response\n",
    "    temperature=0.5  # Add some randomness to help generate responses\n",
    ")\n",
    "\n",
    "# Define the workflow steps - in German since we're using a German model\n",
    "workflow_steps = [\n",
    "    \"Montieren Sie die Halterung und das notwendige ZubehÃ¶r.\",\n",
    "    \"Positionieren und schlieÃŸen Sie das CARDIOHELP-GerÃ¤t an.\",\n",
    "    \"Verbinden Sie die Sensoren.\",\n",
    "    \"Schalten Sie das CARDIOHELP-GerÃ¤t ein und lassen Sie es einen Selbsttest durchfÃ¼hren.\"\n",
    "]\n",
    "\n",
    "current_step = 0\n",
    "user_language = \"German\"\n",
    "\n",
    "def interpret_user_response(user_input):\n",
    "    \"\"\"Simplified function that doesn't rely on model interpretation\"\"\"\n",
    "    # Simple pattern matching instead of using the model\n",
    "    user_input = user_input.lower()\n",
    "    \n",
    "    # List of German words indicating completion\n",
    "    confirmation_words = [\n",
    "        \"fertig\", \"erledigt\", \"abgeschlossen\", \"ok\", \"ja\",\n",
    "        \"gemacht\", \"geschafft\", \"bereit\", \"weiter\"]\n",
    "    \n",
    "    # Check if any confirmation word is in the user input\n",
    "    for word in confirmation_words:\n",
    "        if word in user_input:\n",
    "            #print(f\"Confirmation detected: '{word}' in '{user_input}'\")\n",
    "            return \"yes\"\n",
    "    \n",
    "    return \"no\"\n",
    "\n",
    "while current_step < len(workflow_steps):\n",
    "    step_instruction = workflow_steps[current_step]\n",
    "    \n",
    "    # Simplified German system prompt\n",
    "    system_prompt = f\"\"\"\n",
    "    Du bist ein hilfreicher Assistent fÃ¼r die Einrichtung des CARDIOHELP-MedizingerÃ¤ts.\n",
    "    \n",
    "    Aktueller Status: Schritt {current_step + 1} von {len(workflow_steps)}\n",
    "    \n",
    "    Sei prÃ¤zise und klar in deinen Anweisungen.\n",
    "    \"\"\"\n",
    "\n",
    "    # Custom prompt formatting based on the model type\n",
    "    # Try a simple format first\n",
    "    full_prompt = f\"<s>Bitte gib Anweisungen fÃ¼r Schritt {current_step + 1}: {step_instruction}</s>\"\n",
    "    \n",
    "    try:\n",
    "        # Generate a response\n",
    "        response = llm(\n",
    "            full_prompt,\n",
    "            max_tokens=256,\n",
    "            temperature=0.5,  # Higher temperature for more creative responses\n",
    "            stop=[\"</s>\", \"<s>\"]  # Common stop tokens\n",
    "        )\n",
    "\n",
    "        response = response['choices'][0]['text'].strip()\n",
    "        \n",
    "        # Print whatever we get back, regardless of format\n",
    "        #print(f\"\\nSchritt {current_step + 1}:\")\n",
    "        print(response if isinstance(response, str) else str(response))\n",
    "        \n",
    "        # For testing purposes, always provide a confirmation\n",
    "        user_input = random.choice([\"fertig\", \"erledigt\", \"abgeschlossen\", \"ok\"])\n",
    "        print(f\"-- Benutzer: {user_input}\")\n",
    "        \n",
    "        # Using our simpler interpretation function\n",
    "        model_decision = interpret_user_response(user_input)\n",
    "        #print(f\"Entscheidung: {model_decision}\")\n",
    "        \n",
    "        # Always move to the next step in test mode\n",
    "        current_step += 1\n",
    "        #print(\"NÃ¤chster Schritt...\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating response: {e}\")\n",
    "        # Increment step to avoid infinite loop in case of error\n",
    "        current_step += 1\n",
    "\n",
    "print(\"Einrichtung abgeschlossen!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'genai-job-agents-5rfBW_Ap-py3.12 (Python 3.12.6)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/lilly/Library/Caches/pypoetry/virtualenvs/genai-job-agents-5rfBW_Ap-py3.12/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# example how to run the tts \n",
    "\n",
    "# # From your parent directory script:\n",
    "# from tts import generate_speech\n",
    "\n",
    "# # Generate speech\n",
    "# output = generate_speech(\n",
    "#     text=\"ÃœberprÃ¼fen Sie ob alle in der Lieferungen enthaltenen Komponenten vorhanden sind.\",\n",
    "#     output_file=\"output.wav\",\n",
    "#     reference_audio=\"Germany_second.wav\",  # Note: path is relative to where you run the script\n",
    "#     language=\"de\"\n",
    "# )\n",
    "\n",
    "# print(f\"Speech generated and saved to {output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
